<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        Document
    </title>
    <link rel='stylesheet' href=../../../css/prism.css /><link rel='stylesheet' href=../../../css/index.css />
    <link rel="icon" href="https://raw.githubusercontent.com/learner-lu/picbed/master/logo.png">
</head>

<body class="light">
    <div class="header-navigator"><ul><li><a href="#h1-0">内存管理</a><ul><li><a href="#h2-1">物理内存布局探测</a></li></ul><ul><li><a href="#h2-2">memblock</a></li></ul><ul><li><a href="#h2-3">SMP</a><ul><li><a href="#h3-4">UMA</a></li></ul><ul><li><a href="#h3-5">NUMA</a></li></ul><ul><li><a href="#h3-6">CPU</a></li></ul></li></ul><ul><li><a href="#h2-7">NUMA系统</a><ul><li><a href="#h3-8">NUMA互联</a></li></ul><ul><li><a href="#h3-9">NUMA亲和性</a></li></ul><ul><li><a href="#h3-10">Firmware接口</a></li></ul></li></ul><ul><li><a href="#h2-11">node 初始化</a></li></ul><ul><li><a href="#h2-12">参考</a></li></ul></li></ul></div><div class='markdown-body'><h1 id="h1-0">内存管理</h1><p>设计电脑总是一种妥协.计算机的四个基本部件——中央处理单元(CPU)或处理器,内存,存储器和连接部件的电路板(I/O总线系统)——被尽可能巧妙地组合在一起,以创造出一台既划算又强大的机器.设计过程主要涉及处理器(协处理器,多核设置),内存类型和数量,存储(磁盘,文件系统)以及价格的优化.协同处理器和多核架构背后的理念是,在尽可能小的空间中,将操作分配到尽可能多的单个计算单元,并使并行执行计算指令更容易获得和负担得起.就内存而言,这是一个可以由单个计算单元处理的数量或大小的问题,以及哪种内存类型具有尽可能低的延迟.存储属于外部内存,其性能取决于磁盘类型,正在使用的文件系统,线程,传输协议,通信结构和附加的内存设备的数量.</p><p>内存管理是 Linux 非常重要也非常复杂的一个环节, 本节我们从最基本的物理内存布局探测, 到 memblock slab 等复杂的内存管理方案做一些深入的分析</p><h2 id="h2-1">物理内存布局探测</h2><p>对于一个操作系统, 在启动之初有两个非常关键的问题</p><ul><li>操作系统怎样获取设备总内存大小?</li></ul><ul><li>设备的所有内存,操作系统都可以使用吗?</li></ul><p><b>内存总大小等信息作为设备的关键信息,应该在硬件启动初期就由CPU获得并存储,操作系统只需要通过CPU的相关协定读取即可,这个协定就是BIOS中断</b></p><p>在x86芯片中,探测物理内存布局用的BIOS中断向量是0x15,<b>根据ax寄存器值的不同,有三种常见的方式:0xe820,0x801和0x88.</b></p><pre><code class="language-c">// arch/x86/boot/main.c
void main() {
    // ...
    /* Detect memory layout */
    detect_memory();
    // ...
}</code></pre><p>detect_memory 中依次执行三个 BIOS 中断向 CPU 尝试获取物理内存布局, 这三个函数内部通过 <code>boot_params</code> 将内存的信息带出来</p><blockquote><p>boot_params 由 boot.h 引入, <code>extern struct boot_params boot_params</code></p><p>struct boot_params 定义很长, 位于 arch/x86/include/uapi/asm/bootparam.h</p></blockquote><pre><code class="language-c">// arch/x86/boot/memory.c
void detect_memory(void) {
    detect_memory_e820(); /* 使用e820 BIOS中断获取物理内存布局 */
    detect_memory_e801(); /* 使用e801 BIOS中断获取物理内存布局 */
    detect_memory_88(); /* 使用88 BIOS中断获取物理内存布局 */
}</code></pre><p>其中 e820 需要设置 AX 向量号为 <code>0xe820</code>,</p><pre><code class="language-c">// arch/x86/boot/memory.c
#define SMAP    0x534d4150  /* ASCII "SMAP" */

// Input:
// AX = E820h
// EAX = 0000E820h
// EDX = 534D4150h ('SMAP')
// EBX = continuation value or 00000000h to start at beginning of map
// ECX = size of buffer for result, in bytes (should be &gt;= 20 bytes)
// ES:DI -&gt; buffer for result (see #00581)
// int 0x15
static void detect_memory_e820(void)
{
    int count = 0;
    struct biosregs ireg, oreg;
    struct boot_e820_entry *desc = boot_params.e820_table;
    static struct boot_e820_entry buf; /* static so it is zeroed */
    initregs(&ireg);
    ireg.ax  = 0xe820;
    ireg.cx  = sizeof(buf);
    ireg.edx = SMAP;
    ireg.di  = (size_t)&buf;
    /*
     * Note: at least one BIOS is known which assumes that the
     * buffer pointed to by one e820 call is the same one as
     * the previous call, and only changes modified fields.  Therefore,
     * we use a temporary buffer and copy the results entry by entry.
     *
     * This routine deliberately does not try to account for
     * ACPI 3+ extended attributes.  This is because there are
     * BIOSes in the field which report zero for the valid bit for
     * all ranges, and we don't currently make any use of the
     * other attribute bits.  Revisit this if we see the extended
     * attribute bits deployed in a meaningful way in the future.
     */
    // Ouput:
    // CF clear if successful
    // EAX = 534D4150h ('SMAP')
    // ES:DI buffer filled
    // EBX = next offset from which to copy or 00000000h if all done
    // ECX = actual length returned in bytes
    // CF set on error
    // AH = error code (86h) (see #00496 at INT 15/AH=80h)
    do {
        intcall(0x15, &ireg, &oreg);
        ireg.ebx = oreg.ebx; /* for next iteration... */
        /* BIOSes which terminate the chain with CF = 1 as opposed
           to %ebx = 0 don't always report the SMAP signature on
           the final, failing, probe. */
        if (oreg.eflags & X86_EFLAGS_CF)
            break;
        /* Some BIOSes stop returning SMAP in the middle of
           the search loop.  We don't know exactly how the BIOS
           screwed up the map at that point, we might have a
           partial map, the full map, or complete garbage, so
           just return failure. */
        if (oreg.eax != SMAP) {
            count = 0;
            break;
        }
        *desc++ = buf;
        count++;
    } while (ireg.ebx && count &lt; ARRAY_SIZE(boot_params.e820_table));
    boot_params.e820_entries = count;
}

static void detect_memory_e801(void)
{
    struct biosregs ireg, oreg;
    initregs(&ireg);
    ireg.ax = 0xe801;
    intcall(0x15, &ireg, &oreg);
    if (oreg.eflags & X86_EFLAGS_CF)
        return;
    /* Do we really need to do this? */
    if (oreg.cx || oreg.dx) {
        oreg.ax = oreg.cx;
        oreg.bx = oreg.dx;
    }
    if (oreg.ax &gt; 15*1024) {
        return; /* Bogus! */
    } else if (oreg.ax == 15*1024) {
        boot_params.alt_mem_k = (oreg.bx &lt;&lt; 6) + oreg.ax;
    } else {
        /*
         * This ignores memory above 16MB if we have a memory
         * hole there.  If someone actually finds a machine
         * with a memory hole at 16MB and no support for
         * 0E820h they should probably generate a fake e820
         * map.
         */
        boot_params.alt_mem_k = oreg.ax;
    }
}

static void detect_memory_88(void)
{
    struct biosregs ireg, oreg;
    initregs(&ireg);
    ireg.ah = 0x88;
    intcall(0x15, &ireg, &oreg);
    boot_params.screen_info.ext_mem_k = oreg.ax;
}</code></pre><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230628161738.png" alt="20230628161738"></p><ul><li>操作系统怎样获取设备总内存大小?<p>答:通过BIOS 0x15中断,常见有E820,E801和E88子中断号.</p></li></ul><ul><li>设备的所有内存,操作系统都可以使用吗?<p>答:不是的,只有内存类型为usable的才能被操作系统所使用.</p></li></ul><h2 id="h2-2">memblock</h2><p>memblock 子系统主要用于引导过程中的物理内存管理,特别是在早期的启动阶段,当内核尚未完全初始化和建立内存管理器时.一旦内核初始化完成,memblock 子系统的功能通常会被更高级的内存管理机制所取代,如 buddy allocator(伙伴系统)或 slab allocator(SLAB 系统)</p><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230628182631.png" alt="20230628182631"></p><p>memblock 的功能主要包括</p><ul><li>内存块的描述:memblock 维护了一个物理内存块的列表,每个内存块由起始地址和大小来描述.通过这些描述信息,内核可以了解系统中可用的物理内存块的位置和大小.</li></ul><ul><li>内存块的分配和释放:memblock 提供了函数来动态分配和释放物理内存块.内核可以使用这些函数来管理物理内存资源,例如在启动过程中分配页表,初始化内核堆栈等.</li></ul><ul><li>物理内存的管理:memblock 子系统还提供了一些函数来管理物理内存的属性和映射关系,例如设置内存块的属性(例如可读写,可执行等),查询内存块是否被保留等.</li></ul><pre><code class="language-c">/**
 * struct memblock - memblock allocator metadata
 * @bottom_up: is bottom up direction? 用于判断记录的内存是否从底部往顶部增长
 * @current_limit: physical address of the current allocation limit 当前内存管理器管理的物理地址上限
 * @memory: usable memory regions 操作系统可用内存,即E820探测物理布局时,flags为usable的内存区域
 * @reserved: reserved memory regions 在boot阶段保留的内存,包括E820探测物理布局时,flags为reserved的内存区域,boot阶段分配出去的内存区域
 */
struct memblock {
    bool bottom_up;  /* is bottom up direction? */
    phys_addr_t current_limit;
    struct memblock_type memory;
    struct memblock_type reserved;
};</code></pre><pre><code class="language-c">/**
 * struct memblock_type - collection of memory regions of certain type
 * @cnt: number of regions 记录的内存区域(memblock_region)的数量
 * @max: size of the allocated array 最多能使用的内存区域数,当预留的内存区域不足时,管理器会扩展
 * @total_size: size of all regions 所有内存区域的内存之和
 * @regions: array of regions 内存区域数组,每一项代表usable或保留的内存区域
 * @name: the memory type symbolic name 内存管理器类型的名称,例如"memory","reserved"等
 */
struct memblock_type {
    unsigned long cnt;
    unsigned long max;
    phys_addr_t total_size;
    struct memblock_region *regions;
    char *name;
};</code></pre><pre><code class="language-c">/**
 * struct memblock_region - represents a memory region
 * @base: base address of the region 内存区域的起始地址,类型为u64或u32,表示64位/32位架构的支持最大地址长度
 * @size: size of the region 内存区域的大小
 * @flags: memory region attributes 内存区域的类型表示,有四种类型:MEMBLOCK_NONE(普通内存),MEMBLOCK_HOTPLUG(可热拔插内存),MEMBLOCK_MIRROR(镜像内存),MEMBLOCK_NOMAP(非内核直接映射内存),相同类型的相邻内存,条件合适时可以被合并
 * @nid: NUMA node id 暂时略去与NUMA相关的内容
 */
struct memblock_region {
    phys_addr_t base;
    phys_addr_t size;
    enum memblock_flags flags;
#ifdef CONFIG_NUMA
    int nid;
#endif
};


/**
 * enum memblock_flags - definition of memory region attributes
 * @MEMBLOCK_NONE: no special request
 * @MEMBLOCK_HOTPLUG: memory region indicated in the firmware-provided memory
 * map during early boot as hot(un)pluggable system RAM (e.g., memory range
 * that might get hotunplugged later). With "movable_node" set on the kernel
 * commandline, try keeping this memory region hotunpluggable. Does not apply
 * to memblocks added ("hotplugged") after early boot.
 * @MEMBLOCK_MIRROR: mirrored region
 * @MEMBLOCK_NOMAP: don't add to kernel direct mapping and treat as
 * reserved in the memory map; refer to memblock_mark_nomap() description
 * for further details
 * @MEMBLOCK_DRIVER_MANAGED: memory region that is always detected and added
 * via a driver, and never indicated in the firmware-provided memory map as
 * system RAM. This corresponds to IORESOURCE_SYSRAM_DRIVER_MANAGED in the
 * kernel resource tree.
 */
enum memblock_flags {
    MEMBLOCK_NONE       = 0x0,  /* No special request */
    MEMBLOCK_HOTPLUG    = 0x1,  /* hotpluggable region */
    MEMBLOCK_MIRROR     = 0x2,  /* mirrored region */
    MEMBLOCK_NOMAP      = 0x4,  /* don't add to kernel direct mapping */
    MEMBLOCK_DRIVER_MANAGED = 0x8,  /* always detected via a driver */
};</code></pre><h2 id="h2-3">SMP</h2><p>提高硬件性能最简单,最便宜的方法之一是在主板上放置多个 CPU.这可以通过让不同的 CPU 承担不同的作业(非对称多处理)或让它们全部并行运行来完成相同的作业(对称多处理,又名 SMP)来完成.有效地进行非对称多处理需要有关计算机应执行的任务的专业知识,而这在 Linux 等通用操作系统中是不可用的.另一方面,对称多处理相对容易实现.</p><blockquote><p>相对容易但并不是真的很容易.在对称多处理环境中,CPU 共享相同的内存,因此在一个 CPU 中运行的代码可能会影响另一个 CPU 使用的内存.无法再确定在上一行中设置为某个值的变量仍然具有该值;显然,这样的编程是不可能的.</p></blockquote><p>Symmetrical Multi-Processing,简称SMP,即<b>对称多处理技术</b>,是指将多CPU汇集在同一总线上,各CPU间进行内存和总线共享的技术.将同一个工作平衡地(run in parallel)分布到多个CPU上运行,该相同任务在不同CPU上共享着相同的物理内存.</p><p>与 SMP 相对应的还有一个叫做 AMP(Asymmetric Multiprocessing), 即非对称多处理器架构的概念.</p><ul><li>SMP的多个处理器都是同构的,使用相同架构的CPU;而AMP的多个处理器则可能是异构的.</li></ul><ul><li>SMP的多个处理器共享同一内存地址空间;而AMP的每个处理器则拥有自己独立的地址空间.</li></ul><ul><li>SMP的多个处理器操通常共享一个操作系统的实例;而AMP的每个处理器可以有或者没有运行操作系统, 运行操作系统的CPU也是在运行多个独立的实例.</li></ul><ul><li>SMP的多处理器之间可以通过共享内存来协同通信;而AMP则需要提供一种处理器间的通信机制.</li></ul><blockquote><p>现今主流的x86多处理器服务器都是SMP架构的, 而很多嵌入式系统则是AMP架构的</p></blockquote><p>在现行的SMP架构中,发展出三种模型:UMA,NUMA和COMA.下面将介绍前两种模型,并详细介绍一下 NUMA</p><blockquote><p>下文讨论的 CPU 是指物理 CPU , 而不是多核 CPU</p></blockquote><h3 id="h3-4">UMA</h3><p>Uniform Memory Access,简称UMA, 即均匀存储器存取模型.<b>所有处理器对所有内存有相等的访问时间</b></p><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/uma-architecture.png" alt="uma-architecture"></p><p>既然要连接多个 CPU 和内存, 这种 UMA 的方式很明显是最简单直接的, 但问题也同样明显, BUS 会成为性能的杀手. 多个 CPU 需要平分总线的带宽, 这显然非常不利于计算</p><p>x86多处理器发展历史上,早期的多核和多处理器系统都是UMA架构的.这种架构下, 多个CPU通过同一个北桥(North Bridge)芯片与内存链接.北桥芯片里集成了内存控制器(Memory Controller),</p><p>下图是一个典型的早期 x86 UMA 系统,四路处理器通过 FSB (前端系统总线, Front Side Bus) 和主板上的内存控制器芯片 (MCH, Memory Controller Hub) 相连,DRAM 是以 UMA 方式组织的,延迟并无访问差异. CPU 通过 PCH 访问内存</p><blockquote><p><a href="https://en.wikipedia.org/wiki/Platform_Controller_Hub" target="_blank">PCH(Platform Controller Hub)</a> 是 Intel 于 2008 年起退出的一系列晶片组,用于取代以往的 I/O Controller Hub(ICH). PCI和PCH在计算机系统中扮演不同的角色,PCI提供了扩展插槽,允许用户通过插入PCI卡来扩展计算机的功能,而PCH则负责管理和控制各种接口和设备的通信.PCI和PCH是不同层次的技术,它们共同工作来实现计算机系统的功能</p><p>SMB(System Management Bus):SMB 是一种系统管理总线,用于连接计算机系统中的各种硬件设备和传感器,以进行系统管理和监控.SMB 主要用于与系统管理芯片(如电源管理,温度传感器,风扇控制等)进行通信,提供系统监控,电源管理,硬件调整等功能.<b>它在系统级别提供了对硬件设备的管理和监控功能,而不是直接用于CPU访问内存</b></p></blockquote><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/numa-fsb-3.png" alt="numa-fsb-3"></p><h3 id="h3-5">NUMA</h3><p>基于总线的计算机系统有一个瓶颈——有限的带宽会导致可伸缩性问题.系统中添加的CPU越多,每个节点可用的带宽就越少.此外,添加的CPU越多,总线就越长,因此延迟就越高. 因此,AMD 在引入 64 位 x86 架构时,实现了 NUMA 架构.</p><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/numa-architecture.png" alt="numa-architecture"></p><p>与UMA不同的是,<b>在NUMA中每个处理器有属于自己的本地物理内存(local memory),对于其他CPU来说是远程物理内存(remote memory)</b>.一般而言,访问本地物理内存由于路径更短,其访存时间要更短.</p><p>之后, Intel 也推出了 x64 的 Nehalem 架构,x86 终于全面进入到 NUMA 时代.x86 NUMA 目前的实现属于 ccNUMA. <b>NUMA 是目前服务器最为广泛使用的模式</b></p><blockquote><p>ccNUMA(Cache Coherent NUMA),即缓存一致性NUMA架构. 这种架构主要是在NUMA架构之上保证了多处理器之间的缓存一致性.降低了系统程序的编写难度.</p><p>Intel 的 CPU 架构演进顺序: 8086/8088架构 | 80286架构 | 80386架构 | 80486架构 | Pentium架构 | Pentium Pro/Pentium II/Pentium III架构 | Pentium 4架构 | Core架构 | Nehalem/Westmere架构 | Sandy Bridge/Ivy Bridge/Haswell架构 | Broadwell/Skylake/Kaby Lake架构 | Coffee Lake/Whiskey Lake/Cascade Lake架构 | Ice Lake/Tiger Lake架构 | Alder Lake架构</p></blockquote><p>从 Nehalem 架构开始,x86 开始转向 NUMA 架构,内存控制器芯片被集成到处理器内部,多个处理器通过 QPI 链路相连,从此 DRAM 有了远近之分. 而 Sandybridge 架构则更近一步,将片外的 IOH 芯片也集成到了处理器内部,至此,内存控制器和 PCIe Root Complex 全部在处理器内部了. 下图就是一个典型的 x86 的 NUMA 架构:</p><blockquote><p>QPI(QuickPath Interconnect)是英特尔(Intel)处理器架构中使用的一种高速互联技术.它用于处理器与其他组件(如内存,I/O设备和其他处理器)之间的通信.</p><p>LLC(Last-Level Cache):LLC 是处理器架构中的最后一级缓存.在多级缓存结构中,处理器通常具有多个级别的缓存,而最后一级缓存(通常是共享的)被称为 LLC.LLC 位于处理器核心和主存之间,用于存储频繁访问的数据,以加快处理器对数据的访问速度.</p><p>MI(Memory Interleaving):MI 是一种内存交错技术,用于提高内存子系统的性能.在内存交错中,内存地址空间被划分为多个连续的区域,并将这些区域分配到不同的物理内存模块中.这样,内存访问可以并行地在多个内存模块之间进行,提供更高的带宽和更快的数据访问速度.</p></blockquote><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/2020-01-09_numa-imc-iio-smb.png" alt="2020-01-09_numa-imc-iio-smb"></p><p>NUMA架构解决了可伸缩性问题,这是它的主要优点之一.在NUMA CPU中,一个节点将拥有更高的带宽或更低的延迟来访问同一节点上的内存(例如,本地CPU在远程访问的同时请求内存访问;优先级在本地CPU上).如果将数据本地化到特定的进程(以及处理器),这将显著提高内存吞吐量.缺点是将数据从一个处理器移动到另一个处理器的成本较高.只要这种情况不经常发生,NUMA系统将优于具有更传统架构的系统</p><h3 id="h3-6">CPU</h3><p>在正式开始介绍 NUMA 之前, 我们需要先介绍一些关于 CPU 的基本术语和概念</p><p>大多数CPU都是在二维平面上构建的.CPU还必须添加集成内存控制器.对于每个CPU核心,有四个内存总线(上,下,左,右)的简单解决方案允许完全可用的带宽,但仅此而已.CPU在很长一段时间内都停滞在4核状态.当芯片变成3D时,在上面和下面添加痕迹允许直接总线穿过对角线相反的CPU.在卡上放置一个四核CPU,然后连接到总线,这是合乎逻辑的下一步.</p><p>如今,每个处理器都包含许多核心,<b>这些核心都有一个共享的片上缓存和片外内存</b>,并且在服务器内不同内存部分的内存访问成本是可变的.</p><p>提高数据访问效率是当前CPU设计的主要目标之一.<b>每个CPU核都被赋予了一个较小的一级缓存(32 KB)和一个较大的二级缓存(256 KB).各个核心随后共享几个MB的3级缓存,其大小随着时间的推移而大幅增长</b>.</p><blockquote><p>为了避免缓存丢失(请求不在缓存中的数据),需要花费大量的研究时间来寻找合适的CPU缓存数量,缓存结构和相应的算法.关于<a href="https://en.wikipedia.org/wiki/Bus_snooping" target="_blank">缓存snoop协议</a>和 <a href="https://www.geeksforgeeks.org/cache-coherence-protocols-in-multiprocessor-system/" target="_blank">缓存一致性</a> 的更详细的解释.</p></blockquote><ul><li>Socket: <b>一个Socket对应一个物理CPU</b>. 这个词大概是从CPU在主板上的物理连接方式上来的,可以理解为 Socket 就是主板上的 CPU 插槽.处理器通过主板的Socket来插到主板上. 尤其是有了多核(Multi-core)系统以后,Multi-socket系统被用来指明系统到底存在多少个物理CPU.</li></ul><ul><li>Core: <b>CPU的运算核心</b>. x86的核包含了CPU运算的基本部件,如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存. 一个Socket里可以有多个Core.如今的多核时代,即使是单个socket的系统, 由于每个socket也有多个core, 所以逻辑上也是SMP系统.<blockquote><p>但是,一个物理CPU的系统不存在非本地内存,因此相当于UMA系统.</p></blockquote></li></ul><ul><li>Uncore: Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore.Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能. 在Nehalem时代,内存控制器被集成到CPU里,叫做iMC(Integrated Memory Controller). 而PCIe Root Complex还做为独立部件在IO Hub芯片里.到了SandyBridge时代,PCIe Root Complex也被集成到了CPU里. 现今的Uncore部分,除了iMC,PCIe Root Complex,还有QPI(QuickPath Interconnect)控制器, L3缓存,CBox(负责缓存一致性),及其它外设控制器.</li></ul><ul><li>Threads: 这里特指CPU的多线程技术.在Intel x86架构下,CPU的多线程技术被称作超线程(Hyper-Threading)技术. Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor), 每个逻辑处理器都有独立的处理器状态,但共享Core内部的计算资源,如ALU,FPU,L1,L2缓存. 这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能,提高了硬件使用效率. x86的超线程技术出现早于NUMA架构.</li></ul><blockquote><p>PCIe Root Complex 是总线的起点和顶层设备.它通常由主板上的北桥芯片或处理器内部的PCIe控制器实现, 是总线架构中的核心组件</p></blockquote><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/Core-and-uncore-in-multicore-processors.png" alt="Core-and-uncore-in-multicore-processors"></p><p>因此, 一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成.每个CPU Core内部又可以由两个CPU Thread组成. 每个CPU thread都是一个操作系统可见的逻辑CPU.对大多数操作系统来说,一个八核HT(Hyper-Threading)打开的CPU会被识别为16个CPU</p><h2 id="h2-7">NUMA系统</h2><p>NUMA体系结构中多了Node的概念,这个概念其实是用来解决core的分组的问题.<b>每个node有自己的内部CPU,总线和内存</b>,同时还可以访问其他node内的内存,NUMA的最大的优势就是可以方便的增加CPU的数量.NUMA系统中,<b>内存的划分是根据物理内存模块和内存控制器的布局来确定的</b></p><p>在Intel x86平台上,所谓本地内存,就是CPU指可以经过Uncore部件里的iMC访问到的内存.而那些非本地的, 远程内存(Remote Memory),则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问.</p><p>与本地内存一样,所谓本地IO资源,就是CPU可以经过Uncore部件里的PCIe Root Complex直接访问到的IO资源. 如果是非本地IO资源,则需要经过QPI链路到该IO资源所属的CPU,再通过该CPU PCIe Root Complex访问. 如果同一个NUMA Node内的CPU和内存和另外一个NUMA Node的IO资源发生互操作,因为要跨越QPI链路, 会存在额外的访问延迟问题</p><blockquote><p>曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示,远程内存访问的延时时本地内存的一倍.</p></blockquote><p>一个NUMA Node内部是由一个物理CPU和它所有的本地内存, 本地IO资源组成的. 通常一个 Socket 有一个 Node,也有可能一个 Socket 有多个 Node.</p><pre><code class="language-bash">root@kamilu:~# sudo apt install numactl

root@kamilu:~# numactl --hardware
available: 1 nodes (0)
node 0 cpus: 0
node 0 size: 914 MB
node 0 free: 148 MB
node distances:
node   0
  0:  10</code></pre><h3 id="h3-8">NUMA互联</h3><p>在Intel x86上,NUMA Node之间的互联是通过 QPI(QuickPath Interconnect) Link的. CPU的Uncore部分有QPI的控制器来控制CPU到QPI的数据访问, 下图就是一个利用 QPI Switch 互联的 8 NUMA Node 的 x86 系统</p><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/numa-imc-iio-qpi-switch-3.png" alt="numa-imc-iio-qpi-switch-3"></p><h3 id="h3-9">NUMA亲和性</h3><p>NUMA Affinity(亲和性)是和NUMA Hierarchy(层级结构)直接相关的.对系统软件来说, 以下两个概念至关重要</p><ul><li>CPU NUMA Affinity<p>CPU NUMA的亲和性是指从CPU角度看,哪些内存访问更快,有更低的延迟.如前所述, 和该CPU直接相连的本地内存是更快的.操作系统如果可以根据任务所在CPU去分配本地内存, 就是基于CPU NUMA亲和性的考虑.因此,CPU NUMA亲和性就是要<b>尽量让任务运行在本地的NUMA Node里.</b></p></li></ul><ul><li>Device NUMA Affinity<p>设备NUMA亲和性是指从PCIe外设的角度看,如果和CPU和内存相关的IO活动都发生在外设所属的NUMA Node, 将会有更低延迟.这里有两种设备NUMA亲和性的问题</p><ol start="1"><li>DMA Buffer NUMA Affinity<p>大部分PCIe设备支持DMA功能的.也就是说,设备可以直接把数据写入到位于内存中的DMA缓冲区. 显然,如果DMA缓冲区在PCIe外设所属的NUMA Node里分配,那么将会有最低的延迟. 否则,外设的DMA操作要跨越QPI链接去读写另外一个NUMA Node里的DMA缓冲区. 因此,<b>操作系统如果可以根据PCIe设备所属的NUMA node分配DMA缓冲区</b>, 将会有最好的DMA操作的性能.</p></li></ol><ol start="2"><li>Interrupt NUMA Affinity<p>当设备完成DMA操作后,它会发送一个中断信号给CPU,通知CPU需要处理相关的中断处理例程(ISR),这个例程负责读写DMA缓冲区的数据. ISR在某些情况下会触发下半部机制(SoftIRQ),以便进入与协议栈(如网络,存储)相关的代码路径,以传输数据. 对大部分操作系统来说,硬件中断(HardIRQ)和下半部机制的代码在同一个CPU上发生. 因此,<b>如果操作系统能够将设备的硬件中断绑定到与操作系统自身所属的NUMA节点相对应的处理器上</b>,那么中断处理函数和协议栈代码对DMA缓冲区的读写操作将会具有更低的延迟.这样做可以减少处理器间的通信延迟,提高系统性能.</p></li></ol></li></ul><blockquote><p>中断处理例程(Interrupt Service Routine, ISR)用于响应硬件中断事件,尽快地处理中断并进行必要的操作. 当一个设备或外部事件触发了一个硬件中断,CPU会中断当前执行的任务,并跳转到相应的ISR来处理中断</p><p>半部机制(SoftIRQ)是一种延迟处理机制,用于处理与中断相关的一些非关键,耗时较长的任务. SoftIRQ不是由硬件中断触发,而是在上下文切换,网络数据包处理,定时器等事件发生时,由内核调度执行的</p></blockquote><h3 id="h3-10">Firmware接口</h3><p>由于NUMA的亲和性对应用的性能非常重要,那么硬件平台就需要给操作系统提供接口机制来感知硬件的NUMA层级结构. 在x86平台,ACPI规范提供了以下接口来让操作系统来检测系统的NUMA层级结构.</p><blockquote><p>ACPI(Advanced Configuration and Power Interface)是一种开放标准,旨在为操作系统和硬件之间提供统一的接口,以实现高级配置和电源管理功能, 包括ACPI表, 系统电源管理, 系统配置和资源管理, 事件处理, 系统配置表和命名空间. ACPI规范的广泛应用使得不同的操作系统和硬件厂商能够以一致的方式进行交互,提高了系统的兼容性和可移植性</p><p>ACPI 5.0a规范的第17章是有关NUMA的章节.ACPI规范里,NUMA Node被第9章定义的Module Device所描述. <b>ACPI规范里用Proximity Domain(接近性域)对NUMA Node做了抽象,两者的概念大多时候等同.</b></p></blockquote><ul><li>SRAT(System Resource Affinity Table) 系统资源关联表, 用于优化资源分配和调度<p>主要描述了系统boot时的CPU和内存都属于哪个Proximity Domain(NUMA Node). 这个表格里的信息时静态的,</p><blockquote><p>如果是启动后热插拔,需要用OSPM的_PXM方法去获得相关信息.</p></blockquote></li></ul><ul><li>SLIT(System Locality Information Table) 系统局部性信息表, 用于优化任务调度和资源分配<p><b>提供CPU和内存之间的位置远近信息</b>.在SRAT表格里,只能告诉给定的CPU和内存是否在一个NUMA Node. 对某个CPU来说,不在本NUMA Node里的内存,即远程内存们是否都是一样的访问延迟取决于NUMA的拓扑有多复杂(QPI的跳数). 总之,<b>对于不能简单用远近来描述的NUMA系统</b>(QPI存在0,1,2等不同跳数), 需要SLIT表格给出进一步的说明.</p><blockquote><p>也是静态表格,热插拔需要使用OSPM的_SLI方法.</p></blockquote></li></ul><ul><li>DSDT(Differentiated System Description Table) 不同化系统描述表, 用于正确识别和管理硬件和设备<p>从 Device NUMA角度看,这个表格给出了系统boot时的外设都属于哪个Proximity Domain(NUMA Node).</p></li></ul><blockquote><p>关于 ACPI 的一些介绍见本系列的 arch/ACPI</p></blockquote><h2 id="h2-11">node 初始化</h2><p>node的初始化,从几条启动日志开始说起</p><p><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230705155010.png" alt="20230705155010"></p><p>通过第一条日志顺藤摸瓜可以找到如下的函数</p><pre><code class="language-c">/**
 * dummy_numa_init - Fallback dummy NUMA init
 *
 * Used if there's no underlying NUMA architecture, NUMA initialization
 * fails, or NUMA is disabled on the command line.
 *
 * Must online at least one node and add memory blocks that cover all
 * allowed memory.  This function must not fail.
 */
// arch/x86/mm/numa.c
static int __init dummy_numa_init(void)
{
    printk(KERN_INFO "%s\n",
           numa_off ? "NUMA turned off" : "No NUMA configuration found");
    /* max_pfn是e820探测到的最大物理内存页,其初始化是max_pfn = e820__end_of_ram_pfn() */
    printk(KERN_INFO "Faking a node at [mem %#018Lx-%#018Lx]\n",
           0LLU, PFN_PHYS(max_pfn) - 1);
    /* 一个nodemask_t是 位图, 最多支持MAX_NUMNODES个node
     * 这里将node 0置位
     */
    node_set(0, numa_nodes_parsed);
    /* 将node 0的起始和结束地址记录起来 */
    numa_add_memblk(0, 0, PFN_PHYS(max_pfn));
    return 0;
}

int __init numa_add_memblk(int nid, u64 start, u64 end)
{
    return numa_add_memblk_to(nid, start, end, &numa_meminfo);
}

// arch/x86/mm/numa_internal.h
struct numa_meminfo {
    int         nr_blks;
    struct numa_memblk  blk[NR_NODE_MEMBLKS];
};

struct numa_memblk {
    u64         start;
    u64         end;
    int         nid;
};</code></pre><p>代码和注释写的很清晰, 当没有 NUMA 架构或者 NUMA 架构被禁止的时候, Linux为了适配两者,将UMA"假装"(fake)成一种NUMA架构,也就只有一个node 0节点,该节点包括所有物理内存. numa_nodes_parsed 为 NUMA 节点的位图, 每一个bit代表一个node,node_set是将一个node设置为"在线".</p><p>numa_add_memblk 就是将一个 node 加入到 numa_meminfo, 并设置内存地址的范围. numa_meminfo, numa_memblk 的结构体也比较清晰</p><blockquote><p>PFN_PHYS(max_pfn) 宏用于获取探测到的最大物理内存页的范围</p></blockquote><p>numa_add_memblk_to 逻辑也比较简单, 就是先做一些配置上的判断, 然后结构体对应元素赋值</p><pre><code class="language-c">static int __init numa_add_memblk_to(int nid, u64 start, u64 end,
                     struct numa_meminfo *mi)
{
    /* ignore zero length blks */
    if (start == end)
        return 0;
    /* whine about and ignore invalid blks */
    if (start &gt; end || nid &lt; 0 || nid &gt;= MAX_NUMNODES) {
        pr_warn("Warning: invalid memblk node %d [mem %#010Lx-%#010Lx]\n",
            nid, start, end - 1);
        return 0;
    }
    if (mi-&gt;nr_blks &gt;= NR_NODE_MEMBLKS) {
        pr_err("too many memblk ranges\n");
        return -EINVAL;
    }
    mi-&gt;blk[mi-&gt;nr_blks].start = start;
    mi-&gt;blk[mi-&gt;nr_blks].end = end;
    mi-&gt;blk[mi-&gt;nr_blks].nid = nid;
    mi-&gt;nr_blks++;
    return 0;
}</code></pre><p>整个系统的函数调用栈如下, numa_init 中也可以看出, 无论是否有 NUMA, 区别只是对所传递的函数指针的调用的差别而已</p><pre><code class="language-c">// arch/x86/kernel/setup.c | setup_arch
// arch/x86/mm/numa_64.c   | initmem_init
// arch/x86/mm/numa.c      | x86_numa_init
// arch/x86/mm/numa.c      | numa_init

void __init x86_numa_init(void)
{
    if (!numa_off) {
#ifdef CONFIG_ACPI_NUMA
        if (!numa_init(x86_acpi_numa_init))
            return;
#endif
#ifdef CONFIG_AMD_NUMA
        if (!numa_init(amd_numa_init))
            return;
#endif
    }
    numa_init(dummy_numa_init);
}</code></pre><h2 id="h2-12">参考</h2><ul><li><a href="https://www.zhihu.com/column/c_1444822980567805952" target="_blank">深入理解Linux内存管理 专栏</a></li></ul><ul><li><a href="https://www.zhihu.com/column/c_1543333099974721536" target="_blank">Linux内存管理 专栏</a></li></ul><ul><li><a href="https://zhou-yuxin.github.io/" target="_blank">zhou-yuxin's blog</a></li></ul><ul><li><a href="https://www.cnblogs.com/linhaostudy/p/12679441.html" target="_blank">内存管理相关数据结构之pg_data_t</a></li></ul><ul><li><a href="https://zhou-yuxin.github.io/articles/2018/Linux物理内存管理——获取物理内存布局,划分内存区与创建NUMA节点/index.html" target="_blank">Linux物理内存管理——获取物理内存布局,划分内存区与创建NUMA节点</a></li></ul><ul><li><a href="https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=09667FE3B088F3927E29EF7518DDA56F?doi=10.1.1.414.3607&rep=rep1&type=pdf" target="_blank">Performance Analysis of UMA and NUMA Models</a></li></ul><ul><li><a href="https://zhuanlan.zhihu.com/p/534989692" target="_blank">理解 NUMA 架构</a></li></ul><ul><li><a href="https://linuxhint.com/understanding_numa_architecture/" target="_blank">linuxhint understanding_numa_architecture</a></li></ul><ul><li><a href="https://zhuanlan.zhihu.com/p/62795773" target="_blank">Linux 内核 101:NUMA架构</a></li></ul><ul><li><a href="https://plantegg.github.io/2021/05/14/十年后数据库还是不敢拥抱NUMA/" target="_blank">十年后数据库还是不敢拥抱NUMA</a>: 好文</li></ul><ul><li><a href="https://houmin.cc/posts/b893097a/" target="_blank">[计算机体系结构]NUMA架构详解</a>: 好文</li></ul><ul><li><a href="https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/" target="_blank">NUMA背后的设计思想</a></li></ul><ul><li><a href="https://en.wikipedia.org/wiki/Bus_snooping" target="_blank">Bus snooping</a></li></ul><ul><li><a href="https://www.geeksforgeeks.org/cache-coherence-protocols-in-multiprocessor-system/" target="_blank">cache-coherence-protocols-in-multiprocessor-system</a></li></ul><ul><li><a href="https://zhuanlan.zhihu.com/p/26078552" target="_blank">NUMA与UEFI</a></li></ul></div>
    <div class="dir-tree"><ul><li><a href="../../md-docs/README" >README</a></li></ul><ul><li><a href="../../linux011/README" >linux011</a><ul><li><a href="../../linux011/README" >README</a></li></ul><ul><li><a href="../../linux011/基础知识概览" >基础知识概览</a></li></ul><ul><li><a href="../../linux011/init" >init</a></li></ul><ul><li><a href="../../linux011/kernel" >kernel</a></li></ul><ul><li><a href="../../linux011/mm" >mm</a></li></ul></li></ul><ul><li><a href="../../前期准备/编译内核" >前期准备</a><ul><li><a href="../../前期准备/编译内核" >编译内核</a></li></ul><ul><li><a href="../../前期准备/调试内核" >调试内核</a></li></ul><ul><li><a href="../../前期准备/邮件订阅" >邮件订阅</a></li></ul><ul><li><a href="../../前期准备/linux目录结构" >linux目录结构</a></li></ul><ul><li><a href="../../前期准备/lfs" >lfs</a></li></ul></li></ul><ul><li><a href="../../linux/namespace-cgroups" >linux</a><ul><li><a href="../../linux/namespace-cgroups" >namespace-cgroups</a></li></ul><ul><li><a href="../../linux/内存管理" >内存管理</a></li></ul></li></ul><ul><li><a href="../../arch/ACPI" >arch</a><ul><li><a href="../../arch/ACPI" >ACPI</a></li></ul></li></ul></div>
    <div class="zood"><a class="" href="https://github.com/luzhixing12345/zood" target="_blank">zood</a></div>
    <script type="text/javascript" src="../../../js/next_front.js"></script><script>addLink("../../linux/namespace-cgroups","../../arch/ACPI","ab")</script><script type="text/javascript" src="../../../js/change_mode.js"></script><script>addChangeModeButton("../../../img/sun.png","../../../img/moon.png")</script><script type="text/javascript" src="../../../js/copy_code.js"></script><script>addCodeCopy("../../../img/before_copy.png","../../../img/after_copy.png")</script><script type="text/javascript" src="../../../js/navigator.js"></script><script type="text/javascript" src="../../../js/prism.js"></script><script type="text/javascript" src="../../../js/picture_preview.js"></script><script type="text/javascript" src="../../../js/check_box.js"></script>
</body>

</html>