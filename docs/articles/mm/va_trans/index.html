<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        Document
    </title>
    <link rel='stylesheet' href=../../../css/index.css />
    <link rel='stylesheet' href=../../../css/c.css /><link rel='stylesheet' href=../../../css/diff.css /><link rel='stylesheet' href=../../../css/json.css /><link rel='stylesheet' href=../../../css/makefile.css /><link rel='stylesheet' href=../../../css/shell.css /><link rel='stylesheet' href=../../../css/txt.css />
    <link rel="icon" href="https://raw.githubusercontent.com/learner-lu/picbed/master/logo.png">
</head>

<body class="light">
    <a href="https://github.com/luzhixing12345/klinux" target="_blank" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <div class="header-navigator"><ul><li><a href="#h1-0">虚拟地址转换</a><ul><li><a href="#h2-1">什么是虚拟内存</a></li></ul><ul><li><a href="#h2-2">物理地址和虚拟地址</a></li></ul><ul><li><a href="#h2-3">页表</a><ul><li><a href="#h3-4">页命中与缺页</a></li></ul><ul><li><a href="#h3-5">flag</a></li></ul></li></ul><ul><li><a href="#h2-6">地址翻译</a></li></ul><ul><li><a href="#h2-7">多级页表</a></li></ul><ul><li><a href="#h2-8">TLB</a><ul><li><a href="#h3-9">为什么需要 TLB shootdown</a></li></ul><ul><li><a href="#h3-10">TLB shootdown 的过程</a></li></ul><ul><li><a href="#h3-11">TLB shootdown 的挑战</a></li></ul><ul><li><a href="#h3-12">优化 TLB shootdown</a></li></ul></li></ul><ul><li><a href="#h2-13">参考</a></li></ul></li></ul></div><div class='markdown-body'><h1 id="h1-0">虚拟地址转换</h1><h2 id="h2-1">什么是虚拟内存</h2><p>一个系统中的进程与其他进程共享CPU和主存资源, 然而共享主存会形成一些特殊的挑战, 例如如果太多的进程需要太多的内存, 那么它们中有一些就根本无法运行; 如果某个进程不小心写入了另一个进程使用的内存, 就可能导致另一个进程以某种完全和程序逻辑无关的方式失败</p><p>因此为了更加有效的管理内存并且减少出错情况, 现代操作系统提供了一种对主存的抽象概念: 虚拟内存(Virtual Memory)</p><p>虚拟内存提供了三个重要的能力</p><ul><li>将主存看作一个<b>高速缓存</b>, 只保存活动区域, 根据需要在<b>磁盘和主存</b>之间来回传输数据</li></ul><ul><li>为每个进程提供<b>一致的地址空间</b>, 简化内存管理</li></ul><ul><li>保护每个进程的地址空间不被其他进程破坏</li></ul><p>虚拟内存赋予应用程序强大的能力, 可以创建和销毁内存片(chunk), 将内存片映射到磁盘文件的某个部分, 以及与其他进程共享内存. 比如我们可以通过读写内存位置读或者修改一个磁盘文件的内容, 或者可以加载一个文件的内容到内存中, 而不需要进行显式的复制</p><p>同时虚拟内存也是危险的, 当应用程序引用一个变量, 间接引用一个指针, 或者调用一个诸如 malloc 这样的动态分配程序时, 就会与虚拟内存交互, 如果使用不当可能遇到复杂危险的错误, 例如 &quot;段错误&quot; 或者 &quot;保护错误&quot;</p><h2 id="h2-2">物理地址和虚拟地址</h2><p>前文<a href="../../mm/detect" target="_self">物理布局探测</a>中提到, 计算机上电之后就会读取插入在主板上的内存条, 此时物理的内存颗粒会被操作系统组织成一个有 M 个连续的字节大小的单元组成的数组, 每一个字节都有唯一的物理地址</p><p>CPU访问内存的最自然的方式就是使用物理地址, 这种方式被称为 <b>物理寻址</b>. 下图表示 CPU 读取从物理地址 4 开始的连续 4 个字节, 当 CPU 执行这条加载指令的时候会生成一个有效的物理地址, 和取址长度, 通过内存总线传递给主存; 主存根据地址找到物理地址为 4 的单元, 取出连续的 4 个字节, 并将其返回给 CPU, CPU 将其存放在一个寄存器之中</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230326201733.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230326201733.png" alt="20230326201733"></a></p><p>在早期 PC 上使用的是物理地址, 现代 CPU 使用的虚拟地址的寻址方式. CPU 通过生成一个虚拟地址(VA)来访问主存, 这个虚拟地址在被送到内存总线之前先传递到 CPU 芯片上的 MMU (Memoryy Management Unit)单元, 将一个虚拟地址转换成物理地址, 在传输给内存. 这一步需要 CPU 硬件和操作系统紧密结合, 如下图所示</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230326202802.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230326202802.png" alt="20230326202802"></a></p><p>系统中实际存在的内存空间是<b>物理地址空间</b>, 一共有 M 字节, 其中 M = 2^m, 物理地址空间范围是 {0, 1, ..., M - 1}</p><p>CPU 从一个 n 位地址空间中构建<b>虚拟地址空间</b>, 一共 N 字节, 其中 N = 2^n, 虚拟地址空间的范围是 {0, 1, ..., N-1}</p><p>值得注意的是, <b>虚拟地址空间和物理地址的空间没有什么关系</b>, 物理内存实际上就是电脑上的内存,一般是 8GB 16GB(2^34)那样; 虚拟地址空间则可以很大, 如果是 64 位的虚拟地址空间, 则可以表示大约 2^64 = 16384P 大小的虚拟地址空间</p><blockquote><p>另外并不是 64 位机器所用的是 64 位的虚拟地址空间, 这对于虚拟内存来说实在是太大了, linux目前使用的是48位的虚拟地址空间, 即256TB. windows 使用的是<a href="https://learn.microsoft.com/zh-cn/windows-hardware/drivers/gettingstarted/virtual-address-spaces" target="_blank">47位</a>, 即128TB. 这个空间大小对于物理内存容量已经足够了</p></blockquote><p>虚拟内存可以被看作一个存放在磁盘上的数组, 大小为 N, 每字节都有一个唯一的虚拟地址, 作为到数组的索引. VM 系统通过将虚拟内存分割称为虚拟页(Virtual Page, VP)的大小固定块, 每个虚拟页的大小是 P = 2^p 字节, 类似的物理内存也被分割为物理页(Physical Page, PP), <b>物理页大小和虚拟页一样都是P字节</b> , 物理页也可以称为页帧</p><blockquote><p>Linux 与 Windows 都是采用 4kb作为物理页和虚拟页大小</p></blockquote><p>通常来说由于物理内存有限, 虚拟地址空间要远远大于物理地址空间, 也就是说虚拟页的数量要远远大于物理页的数量, 因此必然不可能将全部的虚拟页面映射到物理页面中, 所以在任意时刻, 虚拟页面的集合都可以分为三个不相交的子集</p><ul><li>未分配的(unallocated): 没有任何数据和该虚拟页面相关联, 不占用任何磁盘空间</li></ul><ul><li>缓存的(cached): 当前虚拟页面已被分配数据, 并且缓存在物理内存中</li></ul><ul><li>未缓存的(uncached): 当前虚拟页面已被分配数据, 但未被缓存在物理内存中</li></ul><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327101505.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327101505.png" alt="20230327101505"></a></p><p>上图中 VP0 VP4 是未分配的页面, VP2 5 7 是已分配并且缓存的, 剩下的是已分配未缓存的</p><hr><p>这里有几个小问题需要解释一下, 上文说虚拟内存可以被看作一个存放在磁盘上的数组, 大小为 N, 也就是按理来说我们希望 N 刚好为磁盘大小, 但实际上计算机组成的磁盘空间是不确定的, 而操作系统 linux 48位, windows 47位, 也就是说实际上虚拟内存已经在操作系统初始化完成之后确定下来了, 256/128TB, 正常来说这个空间对于我的磁盘是足够大的, 假设我有 1TB 的磁盘, 那么多出来的 255/127TB, 也就是最高位并没有用到, 1TB的磁盘根据虚拟页划分, 映射到虚拟内存, 这没有问题</p><p>但是当磁盘空间大于256TB时, 48位的虚拟地址空间就不足了, Linux中如果需要访问大于256TB的磁盘空间,可以使用LVM(逻辑卷管理器)等技术来扩展磁盘空间, 这种特殊情况并不在本文讨论范围之内</p><p>也并不是说磁盘空间大于 256TB 就一定要一次性全部映射到虚拟地址空间, 分段映射也是合理的</p><p>在早期的系统比如 DEC PDP-11上, 虚拟地址空间甚至比物理地址空间还要小, 但使用虚拟地址空间仍然是一个非常有用的机制, 可以大大简化内存管理, 通常来说在现代操作系统上 <b>物理内存空间 M &lt;&lt; 虚拟内存空间 N</b></p><h2 id="h2-3">页表</h2><p>与缓存类似, 虚拟内存系统需要有办法可以判断一个虚拟页是否缓存在 DRAM 中, 并且确定存放在哪一个物理页中. 如果缓存不命中, 那么还需要判断虚拟页应该对应磁盘的哪个位置, 并且需要从物理内存中选择一个牺牲页, 将虚拟页从磁盘复制到 DRAM 替换掉牺牲页</p><p>虚拟内存系统的功能是由软硬件联合提供的, 包括操作系统, MMU(内存管理单元) 中的地址翻译硬件和一个存放在物理内存中的<b>页表</b>(page table)的数据结构, 页表负责保存虚拟内存到物理内存的映射关系, 操作系统负责维护页表的内容, 当地址翻译硬件试图将一个虚拟地址转换到物理地址的时候会读取页表, 然后根据页表中的信息找到对应的物理地址</p><p>下图是一个页表的基本结构, 页表是常驻内存的, 它是一个页表条目(PTE, Page Table Entry)的数组, 每一个页表条目有一个有效位(valid bit)和 m 位的磁盘地址组成</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327180631.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327180631.png" alt="20230327180631"></a></p><blockquote><p>这里的 m 是上文提到过的物理地址空间的大小, M = 2^m</p></blockquote><p>上图中左侧是页表, 我们可以根据其中的索引找到对应的 PTE.</p><ul><li>如果有效位为1则说明该 PTE 构建了一个从 VP 到 PP 的映射, PTE 中的 m 位地址是主存中的物理页号</li></ul><ul><li>如果有效位为0<ul><li>如果 m 位地址不空, 则该 PTE 没有使用, 但已经与一个 VP 绑定了, 即未缓存虚拟页</li></ul><ul><li>如果 m 位地址为空, 则该 PTE 还没有被分配</li></ul></li></ul><p>这里注意要与之前的虚拟页和物理页区分开, 虚拟页是在磁盘中的, 物理页是在 DRAM 中, 页表也是在 DRAM 中, 下图中的映射关系没有改变, 只是借助页表这一数据结构实现了一种映射关系</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327101505.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327101505.png" alt="20230327101505"></a></p><h3 id="h3-4">页命中与缺页</h3><p>当 CPU 想要读 VP2 虚拟页面中的一个字的时候, CPU 将虚拟地址发送给 MMU, MMU 查找页表进行地址转换, 通过某种技术(下面会提到)通过虚拟地址定位到索引, 找到页表中的 PTE2, 发现其有效位为 1, 说明该 PTE 有效, 则取出其中保存的物理内存地址, 找到物理内存中的数据, 如下图所示</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327213010.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327213010.png" alt="20230327213010"></a></p><p>注意<b>页表并不是一个 Hashmap&lt;int, int&gt;, 而是一个数组, 由虚拟页面的页号作为索引 index 对应查找</b></p><p>但如果 CPU 发出一条指令希望读取 VP3, 此时发现有效位为0, 说明 VP3 并未被缓存, 操作系统触发一个缺页异常, 调用内核的缺页异常处理程序, 选择 DRAM 中的一个物理页作为牺牲页, 假设选中的是位于 PP3 的 VP4, 则内核从磁盘复制 VP3 到内存 PP3, 更新 PTE3, 如下图所示</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327232815.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327232815.png" alt="20230327232815"></a></p><p>当异常处理程序返回的时候, CPU会重新启动导致缺页的指令, 把之前导致缺页的虚拟地址重新发送到 MMU, 此时VP3 已经缓存在 DRAM 中了, 所以正常处理执行</p><p>关于虚拟内存有如下的一些相关名词, 磁盘和内存之间传送页被叫做 <b>交换</b> 或者 <b>页面调度</b>, 当有不命中发生的时候才换入页面的调度策略叫做 <b>按需页面调度</b>, 这也是现代所有系统都使用的页面调度策略</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327234122.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230327234122.png" alt="20230327234122"></a></p><p>实际上操作系统为每一个进程都提供了独立的页表, 也就是每一个进程都会对应以一个独立的地址空间, 上图中只展示了 VP 和 PP的部分, 地址翻译的过程对应页表, 我们注意到进程i的 VP2 和进程j的 VP1对应的都是 PP7 的物理页面, 说明多个虚拟页面可以映射到同一个物理页面当中</p><ul><li><b>简化链接</b><p>独立的虚拟地址空间允许每个进程的内存映像使用相同的基本格式, 比如 64 位地址空间中所有代码段都是从 0x400000 开始的, 数据段在代码段之后, 栈段从用户进程的最高地址空间向下生长</p><p>这样的一致性的地址空间大大简化了链接器的设计和实现, 不再需要去关系实际物理内存中的地址映射关系, 而是在一个统一的地址视图中执行程序</p></li></ul><ul><li><b>简化加载</b><p>当我们运行一个可执行文件的时候, 我们希望将其加载到内存中, Linux 加载器只需要简单的为代码段和数据段分配虚拟页, 然后将其标志位置 0, 加载器实际上并不从磁盘复制任何数据到内存, 只是创建一个未缓存的虚拟页, 该进程分配到时间片开始执行之后再通过缺页中断完成加载</p><p>将一组连续的虚拟页映射到任意一个文件的任意位置称为 <b>内存映射</b>, Linux 提供了一个 mmap 的系统调用, 允许应用程序自己做内存映射</p></li></ul><ul><li><b>简化共享</b><p>每个进程有自己的代码, 数据, 堆, 栈, 不与其他进程共享. 但通常来说我们会需要进程共享代码和数据, 比如每个进程都可能需要调用系统调用以及一些 C 标准库的程序, 比如 printf; 操作系统只需要将这部分经常使用的程序加载到内存一次, 让其他所有进程在使用的时候映射到相同的物理页面即可, 而不是为每个进程都创建一份副本</p></li></ul><ul><li><b>简化内存分配</b><p>当程序调用 malloc 希望分配在堆空间分配一块内存的时候, 操作系统只需要分配 k 个连续的虚拟内存页面, 然后将其映射到物理内存中的任意k 个物理页面即可, 虚拟内存页面需要连续, 但是物理内存的页面没有必要连续, 可以由操作系统的内存分配器寻找最合适的位置</p></li></ul><h3 id="h3-5">flag</h3><p>一个现代操作系统应该可以做到对内存系统的完全控制, 包括不允许一个用户进程修改它的只读代码段, 不允许读写其他进程的私有内存, 不允许修改与其他进程共享的虚拟页面(除非共享者都显示的允许)</p><p>就像我们上文提到过的, 操作系统通过虚拟内存提供了独立的地址空间, 而虚拟内存地址到物理内存地址的映射由操作系统和 MMU 控制, 我们可以在 PTE 页表项中添加一些标志位, 可以在地址翻译阶段更加容易的判断权限和合法性</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328141005.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328141005.png" alt="20230328141005"></a></p><p>上图中添加了三个权限标志位</p><ul><li><b>SUP</b>: 进程是否必须运行在内核态的进程才可以访问, 用户态进程只允许访问 SUP=0 的页面</li></ul><ul><li><b>READ</b>: 读权限</li></ul><ul><li><b>WRITE</b>: 写权限</li></ul><p>如果进程执行的过程中发出了一条指令, 希望读取一个页面, 但是操作系统发现该进程没有这个页面的写权限(比如修改 const 变量) 或者不是内核态进程, 则 CPU 触发异常, 并将控制传递给内核中的异常处理程序, 也就是我们编程过程中很常见的, Linux shell 一般将这种异常报告称为 &quot;段错误(segmentation fault)&quot;</p><blockquote><p>当然,上图中原先的有效位被隐藏了, 页表中依然保留了这一位用于判断 PTE 是否有效</p></blockquote><h2 id="h2-6">地址翻译</h2><p>开始本节内容之前我们先列举一下所需的所有符号简写, 希望读者预览一下有一个大致的印象, 如果后文出现了对应的符号可以到这里对照</p><table><tr><th>符号</th><th>描述</th></tr><tr><td style="text-align:center">                N=2^n</td><td style="text-align:center">                虚拟地址空间中的地址数量</td></tr><tr><td style="text-align:center">                M=2^m</td><td style="text-align:center">                物理地址空间中的地址数量</td></tr><tr><td style="text-align:center">                P=2^p</td><td style="text-align:center">                页的大小</td></tr><tr><td style="text-align:center">                PTBR</td><td style="text-align:center">                Page Table Base Register, CPU中的一个控制寄存器, 指向当前页表</td></tr><tr><td style="text-align:center">                VPO</td><td style="text-align:center">                虚拟页面偏移量 offset</td></tr><tr><td style="text-align:center">                VPN</td><td style="text-align:center">                虚拟页号 number</td></tr><tr><td style="text-align:center">                TLB</td><td style="text-align:center">                Translation Lookaside Buffer 快表</td></tr><tr><td style="text-align:center">                TLBI</td><td style="text-align:center">                快表索引 index</td></tr><tr><td style="text-align:center">                TLBT</td><td style="text-align:center">                块表标记 tag</td></tr><tr><td style="text-align:center">                PPO</td><td style="text-align:center">                物理页面偏移量</td></tr><tr><td style="text-align:center">                PPN</td><td style="text-align:center">                物理页号</td></tr><tr><td style="text-align:center">                CO</td><td style="text-align:center">                缓冲块内字节偏移量 cache offset</td></tr><tr><td style="text-align:center">                CI</td><td style="text-align:center">                高速缓冲索引</td></tr><tr><td style="text-align:center">                CT</td><td style="text-align:center">                高速缓冲标记</td></tr></table><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328144336.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328144336.png" alt="20230328144336"></a></p><p>上图展示了 MMU 是如何利用页表实现这种映射的</p><ul><li>首先每个进程中保留着页表的基地址, 需要访存的时候传送给 PTBR, 指向页表的起始地址</li></ul><ul><li>整个虚拟地址跟据页内偏移量(p, 通常4kb, 12位)被分割为 VPN VPO 两部分</li></ul><ul><li>VPN 对应该页表的索引值, VPN = 13, 则对应页表的索引值也是13, 找到与虚拟页面对应的 PTE</li></ul><ul><li>MMU 判断有效位是否为 1<ul><li>如果为 1 则说明该 PTE 已经和物理地址完成映射, 直接取其中地址</li></ul><ul><li>如果为 0 则说明该 PTE 无效, 产生缺页中断, 从磁盘中将 VP 读入 DRAM, 并完成 PTE, 再将有效位置 1, 重新执行该指令</li></ul></li></ul><ul><li>将 PTE 中的 PPN 取出, 与 VPO 结合得到实际的物理地址.</li></ul><blockquote><p>由于虚拟页和物理页都是 P 字节的, 所以实际上 PPO 就是 VPO, 只是完成了 VPN -&gt; PPN 的替换</p></blockquote><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145117.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145117.png" alt="20230328145117"></a></p><p>当页面命中时, 上图中的12345流程分别对应</p><ol start="1"><li>处理器生成一个虚拟地址, 并将其传给 MMU</li></ol><ol start="2"><li>MMU 根据 VPN 找到 PTEA 地址, 从高速缓存/主存中请求该 PTE</li></ol><ol start="3"><li>高速缓存/主存向 MMU 返回 PTE</li></ol><ol start="4"><li>MMU 构造物理地址, 将其传送给高速缓存/主存已请求数据所在的物理地址 PA</li></ol><ol start="5"><li>高速缓存/主存返回所请求的数据字 Data 交给处理器</li></ol><blockquote><p>这个过程完全是由硬件来处理的</p><p>上图中的 PTEA 是 PTE address的缩写, 也可以简写为 PA</p></blockquote><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145435.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145435.png" alt="20230328145435"></a></p><p>当页面不命中的时候, 即有效位为 0, 情况稍微复杂一点</p><ol start="1"><li>处理器生成一个虚拟地址, 并将其传给 MMU</li></ol><ol start="2"><li>MMU 根据 VPN 找到 PTEA 地址, 从高速缓存/主存中请求该 PTE</li></ol><ol start="3"><li>高速缓存/主存向 MMU 返回 PTE</li></ol><ol start="4"><li>PTE 有效位为 0, MMU 触发异常, 传递给 CPU, 由操作系统内核的缺页异常处理程序</li></ol><ol start="5"><li>缺页处理程序通过算法确定出物理内存的牺牲页, 如果这个页面已经被修改了则将其换出到磁盘</li></ol><ol start="6"><li>缺页处理程序将该虚拟页面调入到物理内存, 替换牺牲页, 并更新 PTE</li></ol><ol start="7"><li>缺页处理程序返回到原来的进程, 再次实行导致缺页的指令</li></ol><p>此时 CPU 再次将之前引起缺页异常的虚拟地址发送给 MMU, 由于对应的 PTE 已经被更新了, 所以就可以正常执行了</p><p>实际上在既使用高速缓存, 也是用虚拟内存的系统中, 还有一步关于高速缓存是否命中的情况的讨论, 如下图所示</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328165110.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328165110.png" alt="20230328165110"></a></p><h2 id="h2-7">多级页表</h2><p>对于一个 32 位的地址空间来说, 4KB 的页大小意味着 p = 12, 所以 VPO 是12位, VPN是20位. 页表也要保存在虚拟页中, 即使每一个 PTE 表项只保存物理地址也需要 4B(即不考虑有效位, SUB READ WRITE), 所以一页中最多可以保存的 PTE 数量是 4KB/4B = 1K = 2^{10}, 一共 VPN 是20位, 所以为了保存所有的虚拟页面, 至少需要 2^{20} / 2^{10} = 2^{10} 个虚拟页, 这些虚拟页只是用来保存页表. 这些虚拟页一共是 1K x 4KB = 4MB, 所以这 4MB 的内存空间没有用来保存任何程序相关的数据, 只是用来保存页表. 对于每一个进程来说都需要一个 4MB 内存空间来存储其需要的页表, 这未免有些太占空间了</p><p>而对于一个 64 位的系统来说(地址8B), 每个进程页表占用内存的大小达到了惊人的 2^{43} x 4KB, 即使只使用 48 位的虚拟地址空间, 也需要 2^{27} x 4KB, 这显然是无法接受的</p><p>但事实上, 对于一个 64 位程序来说, 虽然虚拟地址空间很大(256TB), 但实际上我们编写的程序并不需要那么多内存, 也就是说除去虚拟页4KB的12位, 虽然还剩下 52 位, 也就是 2^{52} 个虚拟页需要保存, 但没有必要都把这么多虚拟页同时存在内存中, 最理想的情况是我们只把用到的那几个虚拟页保存在 DRAM 中, 剩下的都不存, 即使突然又要访问另一个虚拟页中的数据, 那么使用缺页中断再将这个页面调入内存即可</p><p>那么很容易想到的一个办法就是, 52 位的虚拟页表索引项太多了, 我们可以将其分割开来, 使用多级页表</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328193243.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328193243.png" alt="20230328193243"></a></p><p>如上图所示, 假设对于一个 48 位的虚拟地址空间, 我们对于高 32 位每隔 p(12位) 做一个划分, 原先的 VPN 被划分为三段(下文简称 VPN1 VPN2 VPN3), 对应三级页表</p><p>其中 VPN1 对应 VPN 的最高12位, 其中索引项是 2^{12} 个, 为了保存这么多索引项我们只需要付出的内存代价是 32KB</p><blockquote><p>64位系统地址 8B, 虚拟页 4KB, 假设每个 PTE 只保留地址(需要8B), 每个虚拟页可以保存 4KB/8B = 2^9 个 PTE, 所以一共需要 2^{12} / 2^9 = 8 个虚拟页, 8 x 4KB = 32KB</p></blockquote><p>由于虚拟地址的使用是接近连续的, 低地址会连续使用到但是高 12 位几乎不会连续使用, 因为高 12 位的加 1 相当于加了 2&lt;&lt;24 的地址空间</p><p>所以一级页表的 PTE 被使用的极少, 基本上一两个 PTE. <b>这是一种巨大的潜在节约</b>, 这意味着我们只需要存有限的必要的一些页表即可</p><p>以上图为例, 其中一级页表中只有 1 个 PTE 是有效的, 我们根据 PTBR0 找到一级页表的位置, 再根据 VPN1 的索引找到对应的 PTE, 这个 PTE 中保存的地址是其指向的二级页表的 PTBR1, 再重复这个过程, 通过 PTBR1 定位到了这个二级页表, 再根据二级页表索引 VPN2 找到其中三个有效的 PTE, 这三个 PTE 分别保存着其指向的三级页表的 PTBR2/3/4, 再定位到三级页表, 再根据三级页表索引 VPN3 定位到 PTE, 这个 PTE 联系了这个虚拟地址代表的虚拟页和物理页的映射, 这个虚拟页实际上才是我们需要使用的. 我们最后再通过这个 PTE 的地址找到物理页 PPN, 然后通过偏移量 VPO 读取到对应的数据. 完整过程如下图所示</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328193929.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328193929.png" alt="20230328193929"></a></p><h2 id="h2-8">TLB</h2><p>前面我们讨论的页表命中的情况, 如下图, 此时 CPU 产生一个虚拟地址, 需要先由 MMU 根据 PTBR 和 VPN 索引访问一次内存找到对应的 PTE, 在根据 PTE 中的地址找到对应的物理地址访存一次得到数据, 这个过程经历了两次访问内存</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145117.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328145117.png" alt="20230328145117"></a></p><p>许多系统都希望可以消除这种开销, 所以在 MMU 中包含了一个关于 PTE 的小型缓存 TLB 快表</p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328165809.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20230328165809.png" alt="20230328165809"></a></p><p>当虚拟地址发送到 MMU 之后, 先根据 P 分离出后 p 位得到 VPO, 这部分是页内偏移量与页表匹配的过程无关, 高位的 VPN 在被 TLB 分为高位和低位, TLB 中有 T=2^t个组, 高位为 TLBT, 低位为 TLBI</p><p>先根据 TLBI 找到 TLB 的对应索引, 如果该 TLB 表项的 TLBT 和这里的 TLBT 相同, 则认为 TLB 命中, 否则不命中;</p><ul><li>如果命中那么就不需要访问内存直接将命中的 TLB 表项发送给 MMU 作为 PTE 即可</li></ul><ul><li>如果没有命中则走之前的流程, 再更新的过程还需要再更新一下 TLB, 类似的根据算法选择一个牺牲页</li></ul><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20240321222006.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20240321222006.png" alt="20240321222006"></a></p><p><a data-lightbox="example-1" href="https://raw.githubusercontent.com/learner-lu/picbed/master/20240321222310.png"><img src="https://raw.githubusercontent.com/learner-lu/picbed/master/20240321222310.png" alt="20240321222310"></a></p><p>tlb shootdown</p><p>TLB shootdown 是一种在多处理器系统中用于确保所有处理器的 Translation Lookaside Buffer(TLB)保持一致性的机制.TLB 是一种特殊的缓存,用于加速虚拟地址到物理地址的转换过程.当系统中的一个处理器修改了页表条目(Page Table Entry, PTE)时,为了确保其他处理器不会使用过时的 TLB 条目,需要执行 TLB shootdown 操作.</p><h3 id="h3-9">为什么需要 TLB shootdown</h3><p>在虚拟化环境中,多个虚拟机(Guest)共享宿主机(Host)的物理硬件资源.当一个虚拟机修改了内存映射(例如,通过 <code>madvise</code> 系统调用指定某个内存区域为 <code>MADV_DONTNEED</code>,释放内存映射),这可能会导致 TLB 中的条目变得无效.为了防止其他虚拟机访问这些已经释放的内存区域,需要通知所有处理器清空它们 TLB 中的相应条目.</p><h3 id="h3-10">TLB shootdown 的过程</h3><ol start="1"><li><b>检测到页表修改</b>:当一个处理器需要修改页表时,它会检测到这个修改可能会影响到其他处理器的 TLB.</li></ol><ol start="2"><li><b>发送 IPI</b>:修改页表的处理器会使用 Inter-Processor Interrupt (IPI) 来通知其他处理器.</li></ol><ol start="3"><li><b>清空 TLB</b>:收到 IPI 的处理器会响应并清空它们的 TLB,确保它们不会使用过时的页表信息.</li></ol><h3 id="h3-11">TLB shootdown 的挑战</h3><p>TLB shootdown 可能会导致性能问题,因为它涉及到处理器间的通信,增加了系统的开销.在高负载或频繁内存操作的场景下,TLB shootdown 的频率可能会增加,从而影响系统性能.</p><h3 id="h3-12">优化 TLB shootdown</h3><p>为了减少 TLB shootdown 的影响,研究人员和工程师们提出了多种优化策略:</p><ul><li><b>批处理</b>:将多个 TLB shootdown 操作合并为一个,减少 IPI 的使用.</li></ul><ul><li><b>自失效 TLB 条目</b>:设计 TLB 条目,使其在检测到页表修改时自动失效,避免需要显式的 shootdown 操作.</li></ul><ul><li><b>上下文相关的清空</b>:仅在必要时清空用户空间的 TLB 条目,减少不必要的 shootdown.</li></ul><ul><li><b>共享 TLB 目录</b>:使用共享数据结构来同步 TLB 状态,减少每个处理器单独清空 TLB 的需要.</li></ul><h2 id="h2-13">参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/65298260" target="_blank">虚拟地址转换[一] - 基本流程</a></li></ul><ul><li><a href="https://blog.csdn.net/huiyeruzhou/article/details/130818548" target="_blank">【程序人生】HelloWorld_从程序到进程</a></li></ul><ul><li><a href="https://blog.csdn.net/hit_shaoqi/article/details/121887459" target="_blank">memory virtualization: shadow page &amp; nest page</a></li></ul><ul><li><a href="https://blog.csdn.net/hx_op/article/details/103980411" target="_blank">内存虚拟化-shadow实现</a></li></ul><p>tlb</p><ul><li><a href="https://zhuanlan.zhihu.com/p/651427497" target="_blank">再议tlb基础知识</a></li></ul><ul><li><a href="https://blog.csdn.net/ByteDanceTech/article/details/104765810" target="_blank">深入理解 Linux 内核--jemalloc 引起的 TLB shootdown 及优化</a></li></ul><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3342195.3387518" target="_blank">Don&#x27;t shoot down TLB shootdowns!</a></li></ul><ul><li><a href="https://zhuanlan.zhihu.com/p/108425561" target="_blank">TLB原理</a></li></ul></div>
    <div class="dir-tree"><ul><li><a href="../../md-docs/README" >README</a></li></ul><ul><li><a href="../../快速开始/编译内核" >快速开始</a><ul><li><a href="../../快速开始/编译内核" >编译内核</a></li></ul><ul><li><a href="../../快速开始/调试内核" >调试内核</a></li></ul><ul><li><a href="../../快速开始/制作linux发行版" >制作linux发行版</a></li></ul><ul><li><a href="../../快速开始/邮件订阅" >邮件订阅</a></li></ul></li></ul><ul><li><a href="../../mm/detect" >mm</a><ul><li><a href="../../mm/detect" >detect</a></li></ul><ul><li><a href="../../mm/va_trans" >va_trans</a></li></ul><ul><li><a href="../../mm/numa" >numa</a></li></ul><ul><li><a href="../../mm/buddy" >buddy</a></li></ul><ul><li><a href="../../mm/slab" >slab</a></li></ul><ul><li><a href="../../mm/compaction" >compaction</a></li></ul><ul><li><a href="../../mm/tier" >tier</a></li></ul><ul><li><a href="../../mm/migration" >migration</a></li></ul><ul><li><a href="../../mm/mm_struct" >mm_struct</a></li></ul></li></ul><ul><li><a href="../../fs/storage" >fs</a><ul><li><a href="../../fs/storage" >storage</a></li></ul><ul><li><a href="../../fs/filesystem" >filesystem</a></li></ul><ul><li><a href="../../fs/fd" >fd</a></li></ul><ul><li><a href="../../fs/inode" >inode</a></li></ul><ul><li><a href="../../fs/directory" >directory</a></li></ul><ul><li><a href="../../fs/disk-layout" >disk-layout</a></li></ul><ul><li><a href="../../fs/mount" >mount</a></li></ul><ul><li><a href="../../fs/vfs" >vfs</a></li></ul><ul><li><a href="../../fs/ext4" >ext4</a></li></ul><ul><li><a href="../../fs/fuse" >fuse</a></li></ul><ul><li><a href="../../fs/distribute-fs" >distribute-fs</a></li></ul></li></ul><ul><li><a href="../../runtime/proc" >runtime</a><ul><li><a href="../../runtime/proc" >proc</a></li></ul><ul><li><a href="../../runtime/vsdo" >vsdo</a></li></ul><ul><li><a href="../../runtime/elf_format" >elf_format</a></li></ul><ul><li><a href="../../runtime/elf_loader" >elf_loader</a></li></ul><ul><li><a href="../../runtime/ld" >ld</a></li></ul></li></ul><ul><li><a href="../../kernel/init" >kernel</a><ul><li><a href="../../kernel/init" >init</a></li></ul><ul><li><a href="../../kernel/rcu" >rcu</a></li></ul><ul><li><a href="../../kernel/workqueue" >workqueue</a></li></ul></li></ul><ul><li><a href="../../arch/cpu" >arch</a><ul><li><a href="../../arch/cpu" >cpu</a></li></ul><ul><li><a href="../../arch/cache" >cache</a></li></ul><ul><li><a href="../../arch/multicore" >multicore</a></li></ul><ul><li><a href="../../arch/cc" >cc</a></li></ul><ul><li><a href="../../arch/mc" >mc</a></li></ul><ul><li><a href="../../arch/bus" >bus</a></li></ul><ul><li><a href="../../arch/interrupt" >interrupt</a></li></ul></li></ul><ul><li><a href="../../proc/schedule" >proc</a><ul><li><a href="../../proc/schedule" >schedule</a></li></ul><ul><li><a href="../../proc/nice" >nice</a></li></ul><ul><li><a href="../../proc/manage" >manage</a></li></ul><ul><li><a href="../../proc/signal" >signal</a></li></ul><ul><li><a href="../../proc/cgroup" >cgroup</a></li></ul><ul><li><a href="../../proc/task_struct" >task_struct</a></li></ul><ul><li><a href="../../proc/rb-tree" >rb-tree</a></li></ul></li></ul><ul><li><a href="../../device/io" >device</a><ul><li><a href="../../device/io" >io</a></li></ul><ul><li><a href="../../device/disk" >disk</a></li></ul><ul><li><a href="../../device/ssd" >ssd</a></li></ul><ul><li><a href="../../device/driver" >driver</a></li></ul><ul><li><a href="../../device/io_uring" >io_uring</a></li></ul></li></ul><ul><li><a href="../../others/safety" >others</a><ul><li><a href="../../others/safety" >safety</a></li></ul><ul><li><a href="../../others/Q&A" >Q&A</a></li></ul></li></ul><ul><li><a href="../../driver/intro" >driver</a><ul><li><a href="../../driver/intro" >intro</a></li></ul></li></ul></div>
    <div class="zood"><a class="" href="https://github.com/luzhixing12345/zood" target="_blank">zood</a></div>
    <script type="text/javascript" src="../../../js/next_front.js"></script><script>addLink("../../mm/detect","../../mm/numa","ab")</script><script type="text/javascript" src="../../../js/change_mode.js"></script><script>addChangeModeButton("../../../img/sun.png","../../../img/moon.png")</script><script type="text/javascript" src="../../../js/copy_code.js"></script><script>addCodeCopy("../../../img/before_copy.png","../../../img/after_copy.png")</script><script type="text/javascript" src="../../../js/navigator.js"></script><script type="text/javascript" src="../../../js/picture_preview.js"></script><script type="text/javascript" src="../../../js/global_js_configuration.js"></script>
</body>

</html>